\let\negmedspace\undefined
\let\negthickspace\undefined
\documentclass[journal]{IEEEtran}
\usepackage[a5paper, margin=10mm, onecolumn]{geometry}
%\usepackage{lmodern} % Ensure lmodern is loaded for pdflatex
\usepackage{tfrupee} % Include tfrupee package
\setlength{\headheight}{1cm} % Set the height of the header box
\setlength{\headsep}{0mm}     % Set the distance between the header box and the top of the text
\usepackage{gvv-book}
\usepackage{gvv}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{txfonts}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{gensymb}
\usepackage{comment}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} 
\usepackage{listings}
% \usepackage{gvv}                                        
\def\inputGnumericTable{}                                 
\usepackage[latin1]{inputenc}                                
\usepackage{color}                                            
\usepackage{array}                                            
\usepackage{longtable}                                       
\usepackage{calc}                                             
\usepackage{multirow}                                         
\usepackage{hhline}                                           
\usepackage{ifthen}                                           
\usepackage{lscape}
\usepackage[a5paper, margin=10mm, onecolumn]{geometry}
\usepackage{graphicx}
\title{Software assignment}
\author{Srihaas Gunda - EE24BTECH11026}
\date{November 2024}

\begin{document}

\maketitle
\section*{\textbf{Introduction}}

In numerical linear algebra, finding the eigenvalues and eigenvectors of a matrix is a fundamental problem. Eigenvalues are used in a variety of fields, including physics, engineering, machine learning, and data science. For symmetric matrices, the 'Jacobi method'
 is a well-known algorithm that can compute eigenvalues by diagonalizing the matrix through a series of orthogonal transformations.

The Jacobi method is particularly effective for symmetric matrices because it guarantees real eigenvalues. It works by iteratively applying rotations to the matrix to make the off-diagonal elements vanish, eventually leading to a diagonal matrix where the diagonal elements are the eigenvalues of the original matrix.


\section{Jacobi Method Overview}

The Jacobi method diagonalizes a symmetric matrix $A$ by successively applying orthogonal rotations. In each iteration, the largest off-diagonal element is targeted, and a rotation is applied to eliminate it. The goal is to transform the matrix into a diagonal form, where the diagonal elements will be the eigenvalues of the original matrix.

Given a symmetric matrix $A$, the Jacobi method follows these key steps:
\begin{enumerate}
    \item Find the largest off-diagonal element $ a_{pq}$, where $p \neq q$.
    \item Compute the rotation angle $\theta$ using the formula:
    \begin{align}
    \theta = \frac{1}{2} \tan^{-1}\left(\frac{2a_{pq}}{a_{pp} - a_{qq}}\right)
    \end{align}
    where $a_{pp} $ and $ a_{qq} $ are the diagonal elements of the matrix.
    \item Construct the rotation matrix $P(\theta)$, which is a matrix that performs the orthogonal rotation. This matrix has the form:
    \begin{align}
    P(\theta) =
    \myvec{
    1 & \cdots & 0 \\
    \vdots & c & -s \\
    \vdots & s & c
    }
    \end{align}
    where $c = \cos(\theta)$ and $ s = \sin(\theta) $, and the matrix is symmetric.
    \item Update the matrix $A$: The matrix  $A$ is updated by applying the rotation matrix $P$. The update is done in two steps:
    \begin{align}
    A' = P^T A P
    \end{align}
    After this update, the off-diagonal element $a_{pq}$ is reduced to zero, and the matrix $A'$ is closer to being diagonal.
    \item Repeat the process until the matrix becomes sufficiently diagonal, i.e., when the off-diagonal elements are smaller than a given tolerance.
\end{enumerate}
The matrix $P(\theta)$ represents a rotation in the $p - q $ plane and has the following structure:

\begin{align}
P(\theta) = 
\myvec{
1 & 0 & 0 & \cdots & 0 \\
0 & \cos(\theta) & \sin(\theta) & \cdots & 0 \\
0 & -\sin(\theta) & \cos(\theta) & \cdots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
}
\end{align}

This matrix is orthogonal, meaning \( P^T P = I \), where $I$ is the identity matrix. Each such rotation reduces the magnitude of the off-diagonal elements, while preserving the diagonal elements. 

\section{Convergence of the Jacobi Method}

One of the key properties of the Jacobi method is its convergence. The method converges to the eigenvalues of the matrix as long as the matrix is symmetric. The main factors that affect the convergence rate are:
\begin{enumerate}
    \item The magnitude of the off-diagonal elements.The algorithm works by zeroing out the largest off-diagonal elements. As these elements become smaller, the matrix gets closer to diagonal form.
    \item The number of iterations: The method typically requires more iterations for larger matrices or matrices with smaller off-diagonal elements. The method converges when all off-diagonal elements fall below a pre-specified tolerance.
    \item The matrix's condition number: A matrix with a large condition number (i.e., a matrix that is ill-conditioned) might require more iterations for convergence.
\end{enumerate}

In practice, the Jacobi method converges relatively slowly, especially for large matrices. It is generally more efficient for small to medium-sized matrices or for problems where other more sophisticated methods (e.g., QR algorithm) are not applicable.


\section{Computational Complexity}

The computational complexity of the Jacobi method can be analyzed by looking at the operations performed in each iteration. Each iteration involves finding the largest off-diagonal element and applying the rotation matrix $P$, which requires:
\begin{enumerate}
 \item Finding the largest off-diagonal element: This involves scanning all $\frac{n(n-1)}{2}$ off-diagonal elements, which takes $O(n^2)$ time.
 \item Constructing the rotation matrix and applying it: Constructing the matrix $P$ and performing matrix multiplications take $O(n^3)$ operations.
\end{enumerate}
Therefore, the overall time complexity per iteration is $O(n^3)$. If the method requires $k$ iterations to converge, the total time complexity is approximately $O(k n^3)$.

The number of iterations $k$ depends on the size and condition of the matrix, and can vary. In practice, the Jacobi method typically requires between 20 and 100 iterations for typical symmetric matrices.

\section{Advantages and Disadvantages}

The Jacobi method has several advantages:
\begin{enumerate}
    \item It is simple to implement.
    \item It guarantees real eigenvalues for symmetric matrices.
    \item It works well for small to medium-sized matrices and in scenarios where other methods might not be applicable.
\end{enumerate}

However, there are also disadvantages:
\begin{enumerate}
    \item It is relatively slow for large matrices.
    \item It requires $O(n^3)$ operations per iteration, making it inefficient for large-scale problems.
    \item The convergence rate is slow compared to other algorithms such as the QR algorithm.
\end{enumerate}


\section{Comparison of Algorithms}

\begin{tabular}{|l|l|l|l|}
\hline
\textbf{Algorithm} & \textbf{Time Complexity} & \textbf{Suitability}  \\ \hline
Jacobi & $O(n^3)$ & Symmetric matrices \\ \hline
QR Decomposition & $O(n^3)$ & General matrices \\ \hline
Power Iteration & $O(kn^2)$ & Sparse matrices \\ \hline
Lanczos & $O(kn^2)$ & Large, sparse matrices \\ \hline
\end{tabular}

\section*{\textbf{Conclusion}}

The Jacobi method is a reliable algorithm for computing eigenvalues and eigenvectors of symmetric matrices, particularly when high precision is required. However, for larger or sparse matrices, alternative methods like Lanczos or QR decomposition may be more suitable.




\end{document}
